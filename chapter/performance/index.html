<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Performance Profiling</title>
  <meta name="description" content="The Open Augmented Reality Teaching Book">

  <link rel="stylesheet" href="/ar-for-eu-book/css/main.css">
  <link rel="canonical" href="https://klamma.github.io/ar-for-eu-book/chapter/performance/">
  <link rel="alternate" type="application/rss+xml" title="The Open Augmented Reality Teaching Book" href="https://klamma.github.io/ar-for-eu-book/feed.xml">

  <!-- Visualization Libraries -->
  <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>
  <script src="https://code.highcharts.com/highcharts.js"></script>
  <script src="https://code.highcharts.com/highcharts-more.js"></script>
  <script src="https://code.highcharts.com/modules/exporting.js"></script>

  <!-- formula rendering -->
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/ar-for-eu-book/">The Open Augmented Reality Teaching Book</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
	  <a class="page-link" href="https://codereality.net/">Code Reality</a>
          <a class="page-link" href="/ar-for-eu-book/posts">Blog Posts</a>
          <a class="page-link" href="/ar-for-eu-book/toc">Table of Content</a>
          <a class="page-link" href="/ar-for-eu-book/references">Bibliography</a>
          <a class="page-link" href="/ar-for-eu-book/contrib">Contributors</a>
          <a class="page-link" href="/ar-for-eu-book/about">About</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Performance Profiling</h1>
  </header>

  <div class="post-content">
    
<h1 id="performance-metrics">Performance Metrics</h1>

<p>Monitoring and optimising the performance of an AR application is important to ensure a pleasant user experience.
Performance profiling of AR applications is not only concerned with the responsiveness of the app.
Instead, developers need to look at further measures like the application’s framerate.
The framerate is a number which counts how many images the real-time graphics application can render per second.
Maintaining stable, high framerates is a necessary requirement for a pleasant usage experience.
In AR and VR applications, low framerates can lead to physical discomfort and dizzyiness in the form of cybersickness.</p>

<p>AR applications are mainly deployed to portable devices like the Microsoft HoloLens and smart phones.
Hence, performance profiling also regards the optimisation of the application’s power consumption to avoid that the device’s battery is drained.
Additionally, developers should monitor the memory footprint of the application.
With real-time graphics applications, an the memory consumption can quickly rise if the developer uses a lot of high-resolution assets like textures or 3D models.</p>

<h1 id="performance-profiling-tools">Performance Profiling Tools</h1>

<p>There are various different tools which allow developers to monitor and analyze the performance of an application.
They are used to identify bottlenecks and narrow down the reason for a restricted performance.</p>

<h2 id="unity-performance-profiler">Unity Performance Profiler</h2>

<p>Unity has its own built-in profiler.
It provides detailed information about the caused CPU load and memory consumption.
Additionally, more details are available about the CPU and memory consumption of the rendering pipeline, the physics simulation and the audio sources in the scene.</p>

<p>The Unity profiler can be found in the Unity editor under “<em>Window &gt; Analysis &gt; Profiler</em>” or by pressing <code class="highlighter-rouge">Ctrl + 7</code>.
In the top bar there is a button “Record” which needs to be set to active so that the profiler actually tracks the performance data.
With the window open, press the play button in the Unity editor to start the application.
The profiler window will start filling with statistics while the application runs.
The profiler window only visualizes the data of the last 300 frames.
This number can be increased in the preferences of Unity.
Hence, one should pause or stop the application’s execution once interesting data have been generated.
In the top bar of the profiler, there is also an option to save the recorded data in a binary format.
This way, they can be inspected again at a later point by loading the generated log file into the profiler again.</p>

<figure class="image">
    <img src="../../assets/figures/performance_profiling/UnityProfiler.png" alt="Unity's profiler window with sample data" />
    <figcaption>Unity's profiler window with sample data</figcaption>
</figure>

<p>Further information about the profiler can be found in <a href="https://docs.unity3d.com/Manual/ProfilerWindow.html">Unity’s documentation about the profiler window</a>.</p>

<p>By default, Unity evaluates the performance of the application while it is running on the development PC.
Since it most likely does not have the same hardware specifications as the target device, e.g. the HoloLens, this will not give an accurate impression of the performance of the deployed application.
However, it is already helpful to profile on the development PC to analyse which components are computationally expensive.</p>

<h3 id="profiling-a-deployed-application-on-a-device-using-unitys-profiler">Profiling a Deployed Application on a Device Using Unity’s Profiler</h3>

<p>Unity’s profiler is also able to record statistics of a deployed application while it is running on the target device.
A prerequisite for remote profiling is that the application is marked as a <em>Development Build</em>.
This option <em>Development Build</em> can be checked in the build window under “<em>File &gt; Build Settings…</em>”.
Once this option is marked, you can also check “Autoconnect Profiler”.
It will write the IP address of the development PC into the application.
Once it is executed, the application will search for the profiler and send data to it.</p>

<p>For the Microsoft HoloLens, the UWP app also requires internet capabilities.
It can be set in the player preferences which can be opened in the inspector by clicking the button “Player settings…” in the build window.
In the tab for Universal Windows Platform settings which displays a Windows logo, the capabilities can be set in the section <em>Publishing Settings</em>.
Check <em>InternetClientServer</em> to allow the application to access network connections and to answer requests by the profiler.</p>

<p>In the top bar of the profiler, there is a dropdown menu which currently says <em>Editor</em>.
If you checked “<em>Autoconnect Profiler</em>”, the running application is available in this dropdown menu.
Otherwise, there is a second option <em>Enter IP address</em>.
Here, you can enter the IP address of the device on which the application is running.</p>

<h2 id="mixed-reality-toolkit-profiler-window">Mixed Reality Toolkit Profiler Window</h2>

<ul>
  <li>how to enable and disable it</li>
  <li>information that it provides</li>
</ul>

<h2 id="visual-studio-profiler">Visual Studio Profiler</h2>

<ul>
  <li>how to access it</li>
  <li>information that it provides</li>
</ul>

<h2 id="hololens-device-portal">HoloLens Device Portal</h2>

<ul>
  <li>how to access it</li>
  <li>information that it provides</li>
</ul>

<h2 id="performance-optimisation">Performance Optimisation</h2>

<p>Detailed optimisation of an application starts with identifying the bottlenecks of the application.
After that, a change is conceptualised and realised.
Finally, the altered version is analysed again to make sure that the optimisation did actually improve the performance.
This high-level process is repeated until the application has an acceptable performance.
Apart from this general workflow, there are some general hints how a Unity application can be optimised:</p>

<h3 id="scene-optimisation">Scene Optimisation</h3>

<p>For HoloLens applications, the Mixed Reality Toolkit provides an optimisation window which automatically optimises the application for AR.
It can be found under “<em>Mixed Reality Toolkit &gt; Utilities &gt; Optimize Window</em>”.
For the best usage experience, all listed recommondations should be accepted and applied.</p>

<p>Apart from this, the following criteria can be considered for optimisation.
To decide which of these recommendations need to be followed, the application should be profiled first.
This way, the bottlenecks can be identified.
For instance, there is no need to reduce the complexity of a mesh if there is script in the scene which does a long calculation each frame.
In this case, the bottleneck is in the application’s logic and not the rendering process.</p>

<p><strong>Avoid computationally intensive shaders</strong>:
Shaders are programs which calculate how the surface of an object should be displayed, e.g. how it is shaded.
Just like scripts, shaders can be written in an inefficient way or provide unnecessary photorealistic results at the cost of performance.
A nice looking visual effect cannot be enjoyed by the user if it slows down the entire application.
Hence, you should check if the shaders slow down the rendering process.
Every material in the scene is based on a shader.
The shader of the material can be changed in its material settings in the inspector using a dropdown at the top:</p>

<p><img src="../../assets/figures/performance_profiling/MaterialShader.png" alt="Shader Selection in the Material" /></p>

<p>By default, materials use the Standard shader.
This shader uses physically-based shading and offers many options but it is not optimsed for mobile devices.
For HoloLens development, it is advisable to restrict the used shaders to the ones which are provided by the Mixed Reality Toolkit.
For mobile platforms, Unity already ships with a series of lightweight shaders.
They can be found in the category “<em>Mobile</em>”.</p>

<p><strong>Reduce the amount of polygons on meshes</strong>:
Meshes consist of vertices which are connected by edges.
The areas which are framed by edges are faces.
Togehter, they define the surface of the object.
In the rendering pipeline, the vertices are first processed by a geometry shader which can manipulate the mesh.
After that, every polgyon is rasterized.
The result of the rasterization are a number of fragments which are candidates for a pixel.
The fragments are processed by a pixel shader.
Only after that, a depth test is performed to discard the fragments which are occluded by meshes in front of them.
This means that every polygon which is potentially visible inside the camera’s view bounds is rendered, even if it is occluded.
The more vertices and polyons an object has, the more work the geometry- and pixel-shaders need to perform.</p>

<p>A large mesh can become a bottleneck for the GPU which has to perform the rendering pipeline.
However, the mesh can also affect the CPU, e.g. if it has to calculate deformations of the meshes, e.g. character animations.
It is necessary to find a balance between a low-resolution mesh which processes fast and a high-resolution mesh which can define smoother curvatures.
The number of polygons can be reduced by removing faces which will never be visible.
As an example, take the 3D model of a camera. If consists of a cuboid and a cylinder.
The cylinder has caps on both sides.
However, the cap which touches the cuboid can be removed since it will never be visible nor relevant for the model
On this paticular model, this simple change removes 32 triangles and one vertex.</p>

<p>It is also important to know that hard edges and UV seams increase the number of vertices.
A vertex is not only represented by its 3D position in space.
It also has a 2D position which defines where it is situated on a texture.
Additionally, it has a normal vector which points outwards and which is perpendicular to the surface’s curvature.
With UV seams, the vertex has two positions on the texture.
With hard eges, the vertex has two different normal vectors.
Since vertices internally can only have one texture position and one normal, this means that the vertex is duplicated.
Both versions of the vertex have the same 3D position but they differ in the texture coordinates or normal vectors.</p>

<p><strong>Reduce the number of objects, use static batching for non-moving objects</strong>:
For each object in the scene, Unity calls the graphics API to perform a draw call.
This has some overhead to it, so a goal is to keep the number of draw calls as low as possible.
Unity already does some optimisations by <em>batching</em> objects.
This combines their meshes into one big mesh which can be processed in one call.
Static batching only works with non-moving GameObjects, e.g. virtual furniture in a scene. 
To mark such objects as suitable for static batching, there is a checkbox <em>Static</em> in the top right of the inspector.</p>

<p><strong>Reduce the number of different materials, re-use materials</strong>:
GameObjects can only be combined by batching if they have the same material.
Additionally, a GameObject where parts of the mesh have different materials, produce one draw call for each material.
Therefore, it is advisable to only use one material per object and to re-use materials.
It might be intuitive for the setup in the inspector since it also reduces the amount of work to set up materials.
However, it also applies to scripting.
If multiple objects should be coloured blue by a script, there is no need to create a new material for each of them and set its colour.
Instead, the script can create one blue material and assign it to all required objects.</p>

<p><strong>Optimise the size of textures</strong>:
Textures should be quadratic and their size should be a power of two, e.g. 256 pixels x 256 pixels, 512 x 512, 1024 x 1024 or 2048, 2048.
Textures which do not comply to this rule are stored in an uncompressed format which increases their memory usage and loading times.</p>

<p>Another factor which can be optimized is the resolution of the texture.
Only objects with intricate details in the texture and which are intended to be in the close focus of the user should have a righ resolution, e.g. 2048 by 2048 pixels or even 4096 by 4096 pixels.
Other textures can be downgraded to lower resolutions.
One good practise in this context is to still save textures at a high resolution in the project and then limit their resolution in their texture settings.
This way, Unity automatically downscales them in the built project and uses the lower resolution in the final application.
Since the original high-resolution version is still available in the project, it is possible to increase the resolution again at a later point if needed.
To do this, select the texture in the assets browser.
This will open the texture’s properties in the inspector on the right.
There is a separate panel where the texture’s <em>Max Size</em> can be set.
These settings can also be altered for all platforms independently by clicking the small icons at the top of the panel.</p>

<p><img src="../../assets/figures/performance_profiling/TextureSettings.png" alt="Max Size in Texture Settings" /></p>

<p><strong>Use shaders which combine multiple textures into one</strong>:
A PNG texture consists of four channels:
Three channels define colours by mixing fractions of red, green and blue.
The fourth channel stores alpha values, e.g. for transparency.
For textures which define the surface colour and transparency of an object, these channels are actually used.
However, opaque objects do not need the alpha channel because there is no transparency.
Hence, it can be used for something else, e.g. to define the roughness of the surface.
The roughness of the surface is a singular value between zero and one and so it can be encoded by one texture channel.
Similarly, PBR materials can define a value that states which parts of the surface are metallic.
This value can also be encoded in a single texture channel.</p>

<p>In general, whenever a surface property is expressed by one floating point value for each point of the surface, it only requires one texture channel.
If there are four different properties, they can be compressed into the four channels of a texture instead of using four separate textures where three channels are useless.
In particular, monochromatic textures (the value is only written to one channel) and greyscale textures (the same value is written to all channels) can be simplified this way.
However, this is only possible if the shader supports this compression.
The shader must know what information are stored in each channel.</p>

<p>In Unity, some of the shaders support and even require such texture compression.
For instance, the shader “Mixed Reality Toolkit/Standard” which is provided with the Mixed Reality Toolkit uses a “Channel Map” which can be activated by checking the corresponding option.
It stores metallic values in the red channel, ambient occlusion (which simulates shadows in creases) in the green channel, the emission intensity in the blue channel and the surface smoothness in the alpha channel.</p>

<p><img src="../../assets/figures/performance_profiling/MRTKChannelMap.png" alt="Channel Map in MRTK Standard Shader" /></p>

<p>The Mixed Reality Toolkit also provides a tool which compresses multiple textures into the channels of one texture.
It can be found under “<em>Mixed Reality Toolkit &gt; Utilities &gt; Texture Combiner</em>”.
Here, one can place textures in the slots for each channel.
The “Input Channel” dropdown menus define which channel of the given texture should be written to the new texture.
If you do not have a texture for one of the properties, the tool can also write a constant value to the new texture’s channel by dragging the slider.
Although the descriptions guide the creation of channel maps for the Mixed Reality Toolkit’s standard shader, one can also use this tool for other shaders which expect other channel combinations.</p>

<p><img src="../../assets/figures/performance_profiling/MRTKTextureCombiner.png" alt="MRTK Texture Combiner Tool" /></p>

<p>This optimisation reduces the initial loading times of the scene because less textures need to be read.
Additionally, this means that less textures need to be kept in memory.</p>

<p><strong>Use primitive colliders, avoid mesh colliders</strong>:
To work with physics calculations, an object needs colliders.
Colliders are used to find out if an object intersects with another object.
This is not only useful for physics simulations like a sphere falling to the ground but also necessary for raycasts.
Raycasts are needed in a couple of situations, e.g. for user interactions to find out if the user points at an UI element.
There are different collider shapes available.
Each shape has its own performance cost for collision detection.
The quickest collision detection is possible with sphere colliders.
To determine if two spheres intersect, the vector between the midpoints of the spheres needs to be calculated.
If the length of the vector is smaller than half the radius of the first sphere plus half the radius of the second sphere, the spheres intersect.
For <em>Box Colliders</em>, the collision detection is more complex since it involves projections and overlapping tests on multiple axes.
Considerable more computation time needs to be spent on <em>Mesh Colliders</em> where <em>Convex</em> is checked.
The highest performance hit produce <em>Mesh Colliders</em> with the <em>Convex</em> option unchecked.
In this case, each triangle of the mesh needs to be checked for intersections individually.
This means that mesh colliders take longer to evaluate the more triangles the mesh has.</p>

<p>Hence, the recommendation is to avoid mesh colliders.
Instead, it can be sufficient to approximate a 3D object by a set of primitive colliders, e.g. by shapes, capsules or boxes.
The number of colliders should also be kept as low as possible.
It is not always necessary to represent the exact shape of the object but instead a hull around it can be sufficient.
For instance, virtual characters can be approximated by one big capsule.
If a mesh collider has to be used, it can be beneficial to create a second mesh with a lower resolution and use it as the mesh collider.</p>

<p><strong>Use Single Pass Stereo Rendering on Head-Mounted Displays and Single Pass Instanced Rendering for Windows Mixed Reality</strong>:
Head-mounted displays have one screen for each eye.
The two screens show two slightly different images with a shifted perspective.
Due to this shift, the user can see the virtual content in 3D with depth perception.
However, this also means that the render engine needs to create two images each frame instad of one.
Without any optimisations, many portions of the render pipeline need to be done twice.
For instance, the number of draw calls, which have a high overhead, doubles.
An optimisation technique is Single Pass Stereo Rendering (<a href="https://blogs.unity3d.com/2017/11/21/how-to-maximize-ar-and-vr-performance-with-advanced-stereo-rendering/">Unity Blog, 2017</a>).
Instead of rendering two separate images, it uses an image texture with a doubled width where both images are created side by side.
The advantage is that the work which is done in the rendering pipeline, e.g. the results of the culling process, only needs to be done once (<a href="https://docs.unity3d.com/Manual/SinglePassStereoRendering.html">Unity Documentation</a>).
A further optimisation is Single Pass Instanced Rendering which can be used for the Hololens (<a href="https://docs.unity3d.com/Manual/SinglePassInstancing.html">Unity Documentation</a>, <a href="https://docs.unity3d.com/Manual/SinglePassStereoRenderingHoloLens.html">Unity Documentation for HoloLens</a>).
The optimisation window of the Mixed Reality Toolkit automatically applies this technique.</p>

<p>It is important to know that both optimisation techniques require compatible shaders that need to support the chosen rendering method.
The shaders of the Mixed Reality Toolkit work with Single Pass Stereo Instanced Rendering.
However, the default shader of the TextMeshPro on Unity 2018 does not work.
The text is only rendered on one eye and is invisible for the other eye.
To solve this, the TextMeshPro object needs to use a text shader which is provided by the Mixed Reality Toolkit.</p>

<h3 id="script-optimisation">Script Optimisation</h3>

<p><strong>Use co-routines or threads for long-lasting or blocking operations</strong>:
Anything logic of a script (except for threaded code) is performed on the main thread.
This main thread is also used by the renderer which draws new frames to the user’s screen.
Therefore, any operations which take a longer time and are blocking, e.g. I/O operations will freeze the application until they are completed.
The user will immediately notice this since no new frames will be rendered until the operation has finished.</p>

<p>The simplest solution to this are <em>coroutines</em>.
They use a concurrency approach to divide bigger operations into many small ones.
In-between the computation of these individual small elements, the operation releases the control of the main thread again.
This gives the renderer and other application components the chance to process the next frame in-between the small chunks of the large operation.</p>

<p>There is also the option to use native <em>C# threads</em> to create a background thread where compute-intensive operations can be executed.
With this approach it is important to note that Unity’s API is not thread-safe and may only be accessed by the main thread.
This means that no calls to methods provided by Unity or Unity objects can be implemented.</p>

<p>Starting with Unity 2018, there a job system was added to execute multithreaded code.
The job system is also used by Unity itself to handle background tasks.
It creates a thread pool with one thread for each processor core.
This optimises the performance because there is no overhead for creating and finishing threads.
Additionally, each processor core works its own thread which means that the cores perform less context switches.
Like normal threads, Unity jobs still have the problem that Unity’s API is not thread-safe and cannot be accessed.
However, Unity introduced unmanaged, native container classes which are thread-safe.
An example for this is the <code class="highlighter-rouge">NativeArray</code>.
Developers can create jobs and schedule them.
After that, they are picked up and executed by one of the available threads.</p>

<p><strong>Avoid <code class="highlighter-rouge">Update()</code></strong>:
The <code class="highlighter-rouge">Update()</code> method is executed every frame.
By moving logic from the <code class="highlighter-rouge">Update()</code> method to other solutions, the processing time of every frame can be improved.
As an example, a script should be implemented which realises a virtual light switch.
There is a virtual button which a script <code class="highlighter-rouge">ToggleButton</code>.
If the user presses the button, a property <code class="highlighter-rouge">IsOn</code> is toggled.
In the example we assume that the input system calls the method <code class="highlighter-rouge">OnUserClick()</code> if user input for the GameObject is detected.
The task is to write a script which can visualize this value with a virtual light.
If <code class="highlighter-rouge">toggleButton.IsOn == true</code>, the virtual light should be on.
The naive way to implement this is the following:</p>

<pre><code class="language-[C#]">// this script is placed on the button
public class ToggleButton : MonoBehaviour
{
    public bool IsOn {get; private set;}

    void OnUserInput()
    {
        IsOn = !IsOn;
    }
}


// this script is placed on the light
public class LightSwitch : MonoBehaviour
{
    // Start is called before the first frame update
    void Start()
    {
        
    }

    // Update is called once per frame
    void Update()
    {
        // get the light component on this GameObject
        Light light = GetComponent&lt;Light&gt;();
        // synchronise the state of the light with the button's IsOn property
        light.enabled = (GameObject.Find("MyButton").GetComponent&lt;ToggleButton&gt;()).IsOn
    }
}
</code></pre>

<p>This script is not optimal in multiple aspects.
Another optimisation of this script is addressed in the next paragraph.
However, this logic does not need to be implemented by using Update().
Instead, events can be used which are only fired if the value changes.
They also have a bit of overhead but since we do not expect that the user is toggling the light, it reduces the amount of code that is executed every frame.</p>

<p><strong>Remove empty callback methods, e.g. <code class="highlighter-rouge">Start()</code> and <code class="highlighter-rouge">Update()</code></strong>:
If you create a new script in Unity, it creates a class that inherits from <code class="highlighter-rouge">MonoBehaviour</code> and with two empty methods <code class="highlighter-rouge">Start</code> and <code class="highlighter-rouge">Update</code>.
While this is convenient, the methods should be removed if they are not used.
Unity will call these methods if they exist and has to perform a context switch to change between internal code and the C# script.
The overhead gets considerable if it has to be done multiple times every frame as it is the case with multiple empty `Update methods on different GameObjects.</p>

<p><strong>Cache the result of queries for other GameObjects or MonoBehaviours</strong>:
Other GameObjects can be found by <code class="highlighter-rouge">GameObject.Find("objName")</code>.
This call searches the entire scene for a GameObject with the given name, so this call will be slow if there are many objects in the scene.
The performance hit is especially big if this is method is executed every frame.
For such cases, it is more efficient to use <code class="highlighter-rouge">GameObject.Find</code> just once in the <code class="highlighter-rouge">Start</code> method and to cache the result in a private variable.
Another solution is to establish the reference directly in the editor by exposing a public variable or adding the <code class="highlighter-rouge">[SerializeField]</code> attribute to a private value.
This way, the variable is shown in the inspector in Unity’s editor and the GameObject can be referenced by dragging and dropping it into the field of the variable.
This is also more stable regarding changes.
If the GameObject is renamed, the drag-and-drop reference is maintained.
In contrast to this, all <code class="highlighter-rouge">GameObject.Find</code> calls in the code need to be updated manually to the new name of the GameObject.</p>

<p>Similar recommendations exist for other calls which fetch references, e.g. <code class="highlighter-rouge">GetComponent()</code> or <code class="highlighter-rouge">GetGameObjectsWithTag()</code>.
They are expensive operations and it is better to cache their result in the script instead of executing them repeatedly.</p>

<p><strong>Do not use Camera.main</strong>:
A pitfall regarding the previous recommendation to minimise the use of reference-fetching methods is <code class="highlighter-rouge">Camera.main</code>.
At a glance, this seems to be a useful public variable where the main camera of the scene was cached.
However, calling <code class="highlighter-rouge">Camera.main</code> triggers a <code class="highlighter-rouge">FindGameObjectsWithTag()</code> every time.
It searches a GameObject with the tag <code class="highlighter-rouge">MainCamera</code> and does not cache the result.
Hence, <code class="highlighter-rouge">Camera.main</code> should be regarded as a reference-fetching method.
It should be used sparingly by calling it once in the initialisation of the script and then caching the result.</p>

<p><strong>Use object pooling to minimise the usage of the garbage collector</strong>:
Unlike more-low level programming languages like C++ where developers need to allocate and release memory manually for their objects, C# has a garbage collector which automates the memory management.
If a new object is created, usually with the <code class="highlighter-rouge">new</code> keyword, memory is allocated.
In regular intervals or if there is no memory left, the garbage collector interrupts the execution of the program.
It analyses which objects are not used anymore and frees the memory that was occupied by these objects.
While this takes away the development effort of manually managing memory, it comes at a performance trade-off.
The gargabe collector has a large overhead and can lead to visible stuttering in the 3D application if a lot of objects need to be cleaned up.
Hence, it is advisable to avoid creating new objects and reusing them instead.
As an example, a list <code class="highlighter-rouge">List&lt;string&gt; myList = new List&lt;string&gt;()</code> can be emptied with <code class="highlighter-rouge">myList.Clear()</code> instead of calling the constructor <code class="highlighter-rouge">myList = new List&lt;string&gt;()</code> again.
Both calls end up with an empty list but the version with the constructor creates a new object, allocates new memory and the garbage collector will clean up the memory occupied by the old list.</p>

<p><code class="highlighter-rouge">Instantiate()</code> and <code class="highlighter-rouge">Destroy()</code> create and remove GameObjects and components.
These operations are expensive because they allocate and deallocate memory and need to register and deregister the new components, callback methods, etc.
Destroyed GameObjects are cleaned up by the garbage collector.
Instead of <code class="highlighter-rouge">Instantiate()</code> and <code class="highlighter-rouge">Destroy()</code>, a technique called object pooling can be implemented.
The number of necessary GameObjects are created once in the beginning and they are added to a collection of unused GameObjects.
Instead of calling <code class="highlighter-rouge">Instantiate()</code> in the code again, the code now requests a GameObject from the pool.
Once the object is not used anymore, it is not destroyed but returned to the pool.</p>

<p>Object pooling cannot only be used for GameObjects but for any object.
As an example, it is an important technique when meshes are created via code.</p>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The Open Augmented Reality Teaching Book</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>The Open Augmented Reality Teaching Book</li>
          <li><a href="mailto:klamma@dbis.rwth-aachen.de">klamma@dbis.rwth-aachen.de</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li>
            <a href="https://twitter.com/AR_FOR_EU"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">AR_FOR_EU</span></a>

          </li>
          <li>
            <a href="/ar-for-eu-book/feed.xml"><span class="icon icon-rss"><?xml version="1.0"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"> 
<svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="128px" height="128px" id="RSSicon" viewBox="0 0 256 256">
<defs>
<linearGradient x1="0.085" y1="0.085" x2="0.915" y2="0.915" id="RSSg">
<stop  offset="0.0" stop-color="#E3702D"/><stop  offset="0.1071" stop-color="#EA7D31"/>
<stop  offset="0.3503" stop-color="#F69537"/><stop  offset="0.5" stop-color="#FB9E3A"/>
<stop  offset="0.7016" stop-color="#EA7C31"/><stop  offset="0.8866" stop-color="#DE642B"/>
<stop  offset="1.0" stop-color="#D95B29"/>
</linearGradient>
</defs>
<rect width="256" height="256" rx="55" ry="55" x="0"  y="0"  fill="#CC5D15"/>
<rect width="246" height="246" rx="50" ry="50" x="5"  y="5"  fill="#F49C52"/>
<rect width="236" height="236" rx="47" ry="47" x="10" y="10" fill="url(#RSSg)"/>
<circle cx="68" cy="189" r="24" fill="#FFF"/>
<path d="M160 213h-34a82 82 0 0 0 -82 -82v-34a116 116 0 0 1 116 116z" fill="#FFF"/>
<path d="M184 213A140 140 0 0 0 44 73 V 38a175 175 0 0 1 175 175z" fill="#FFF"/>
</svg>
</span><span class="username">Subscribe</span></a>

          </li>
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>The Open Augmented Reality Teaching Book</p>
      </div>
    </div>

  </div>

</footer>


    <!-- load visualizations (at the end so that the div elements are known here) -->
    
  </body>

</html>
